{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3812a7f6fb29518"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Linear regression is a foundational algorithm in machine learning, offering a simple yet powerful approach for predicting a quantitative response. It's particularly useful in scenarios where the relationship between the independent variable(s) (features) and the dependent variable (target) is linear. The core idea is to find a linear function that best fits the data points, minimizing the difference between the predicted values and actual values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b0b121c20b26188"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comprehensive Key Concepts in Linear Regression\n",
    "\n",
    "### 1. Model Representation\n",
    "Linear regression models the relationship between a dependent variable and one or more independent variables using a linear approach. The simplest form of linear regression, single-variable (or simple) linear regression, can be represented by the equation:\n",
    "\\[ y = w_1x + w_0 \\]\n",
    "where \\(y\\) is the dependent variable, \\(x\\) is the independent variable, \\(w_1\\) is the slope (coefficient) of the model, and \\(w_0\\) is the y-intercept (bias). This equation can be extended to multiple linear regression, which includes multiple independent variables:\n",
    "\\[ y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n \\]\n",
    "Here, each \\(x_i\\) represents a different independent variable, and each \\(w_i\\) represents the coefficient for that variable.\n",
    "\n",
    "### 2. Cost Functions\n",
    "The cost function quantifies the error between the predicted values by the model and the actual values from the dataset. It is a key component in evaluating the performance of the model. The most commonly used cost functions in linear regression are:\n",
    "\n",
    "- **Mean Squared Error (MSE):**\n",
    "\\[ J(w_0, w_1, ..., w_n) = \\frac{1}{2m} \\sum_{i=1}^{m} (f_w(x^{(i)}) - y^{(i)})^2 \\]\n",
    "where \\(m\\) is the number of training examples, \\(f_w(x^{(i)})\\) is the prediction of the model for the \\(i^{th}\\) example, and \\(y^{(i)}\\) is the actual value.\n",
    "\n",
    "- **Mean Absolute Error (MAE):**\n",
    "\\[ J(w_0, w_1, ..., w_n) = \\frac{1}{m} \\sum_{i=1}^{m} |f_w(x^{(i)}) - y^{(i)}| \\]\n",
    "\n",
    "These cost functions are optimized during training to find the best values for the model parameters.\n",
    "\n",
    "### 3. Gradient Descent\n",
    "Gradient Descent is an optimization algorithm used to minimize the cost function by iteratively updating the model's parameters. The general update equations for the parameters in a linear regression model are:\n",
    "\\[ w_j := w_j - \\alpha \\frac{\\partial}{\\partial w_j} J(w_0, w_1, ..., w_n) \\]\n",
    "for \\(j = 0, 1, ..., n\\), where \\(\\alpha\\) is the learning rate, a hyperparameter that controls the step size during the iteration process.\n",
    "\n",
    "For the simple linear regression case, the update rules for \\(w_0\\) and \\(w_1\\) become:\n",
    "- Update rule for \\(w_0\\) (bias):\n",
    "\\[ w_0 := w_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (f_w(x^{(i)}) - y^{(i)}) \\]\n",
    "- Update rule for \\(w_1\\) (weight):\n",
    "\\[ w_1 := w_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} ((f_w(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}) \\]\n",
    "\n",
    "These equations are applied iteratively to adjust the parameters and minimize the cost function.\n",
    "\n",
    "### 4. Model Evaluation Metrics\n",
    "After training a linear regression model, it's important to evaluate its performance using suitable metrics. Common evaluation metrics include:\n",
    "- **R-squared (Coefficient of Determination):** Measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "- **Root Mean Squared Error (RMSE):** The square root of the MSE, providing an interpretable measure of the average error magnitude.\n",
    "\n",
    "### 5. Regularization\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty term to the cost function. The two most common types of regularization in linear regression are:\n",
    "- **Ridge Regression (L2 regularization):** Adds a penalty equal to the square of the magnitude of coefficients.\n",
    "- **Lasso Regression (L1 regularization):** Adds a penalty equal to the absolute value of the magnitude of coefficients.\n",
    "\n",
    "### 6. Assumptions of Linear Regression\n",
    "Linear regression is based on several key assumptions, including linearity, independence, homoscedasticity, and normal distribution of residuals. Ensuring these assumptions are met is crucial for the model to provide accurate and reliable predictions.\n",
    "\n",
    "This comprehensive overview provides a deeper understanding of the fundamental concepts in linear regression, setting a solid foundation for exploring more advanced topics and techniques in machine learning."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6056306634c38cc4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5af1195bf46a27ce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
